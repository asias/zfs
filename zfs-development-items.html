<html>
<head>
<title>ZFS Development Items</title>
<meta name="keyword" content="zfs, linux"/>
<meta name="description" content="ZFS Development Items." />
<meta name="robots" content="all" />
</head>
<body>
<center>
<img src="images/zfs-linux.png">

<table width="80%" cellpadding=3>
	<tr>
		<td colspan=5>
<p>The following is a list of ZFS on Linux development items.  The
items vary in complexity but they are all available to be worked on
by members of the ZFS on Linux community.  If you are a developer
interested in working on one of these items please start a new 
thread on the <a href="lists.html">zfs-devel</a> mailing list.
Then <a href="http://help.github.com/forking/">fork</a> the
 <a href="http://github.com/behlendorf/zfs">zfs project</a>
at GitHub, create a new branch, and begin your development.
Once you've completed the new feature or bug fix send a
<a href="http://help.github.com/pull-requests/">pull request</a>.
This will start the process of reviewing, discussing, and
merging your changes in to the official code.</p><br>
		</td>
	</tr>
	<tr align=center bgcolor="#aaaaaa">
		<th>Status</th>
		<th>Owner</th>
		<th>Difficulty</th>
		<th>Name</th>
		<th>Description</th>
	</tr>
	<tr bgcolor="#dddddd">
		<td>2.6.32-2.6.38 Supported</td>
		<td>Behlendorf</td>
		<td>Medium</td>
		<td>Kernel Compatibility</td>
		<td>
<p>Supporting a wide rage of Linux kernels is complicated by the fact
that the kernel developers may change the Kernel API for each release.
The SPL and ZFS packages cope with this by including a complicated
autoconf infrastructure which probes your kernel at configure time
to determine which APIs to use.  Right now the ZPL has been coded
against the Linux 2.6.32 kernel API which because this is the version
of the kernel used currently by most major distributions.  To support
newer kernels additional autoconf checks must be added to detect and
correctly use both versions of the API.  My initial inclination is to
add support for 2.6.32 and newer kernels, but not to support earlier
kernels.  If this becomes an issue I'm not opposed to adding code to
support older kernels.  But with the official release of RHEL6, Debian
6, and other major distributions using a newer kernel to get a new
feature like ZFS is reasonable.
		</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>Medium</td>
		<td>NFS ACLs</td>
		<td>
<p>Internally ZFS fully supports and enforces NFS style ACLs.  Unfortunately,
under Linux the existing tools only manipulate Posix style ACLs.  There
has been some work done to the bring NFS ACL model to Linux under the
name <a href="http://acl.bestbits.at/richacl/">Rich-ACLs</a>
(<a href="http://www.kernel.org/doc/ols/2010/ols2010-pages-19-32.pdf">pdf</a>).
To integrate with the new Rich-ACL tool chain ZFS needs to provide a
virtual <i>system.richacl</i> xattr interface.  This xattr would not be
stored like other xattrs but would instead integrate with the zfs_getacl()
and zfs_setacl().  This xattr hook would be responsible for translating
a vsecattr_t to and from a lineal stream of bytes for the xattr.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Medium</td>
		<td>Posix ACLs</td>
		<td>
<p>Posix ACLs can be easily supported by adding a few hooks and leveraging
the existing Posix ACL support functions.  However, it may be best that
they <i>not</i> be implemented to avoid coherency issues between Posix
and Rich ACLs (NFS/ZFS).</p>
		</td>
	</tr>
	<tr>
		<td>Kernel nfsd Supported</td>
		<td>Beutner</td>
		<td>Medium</td>
		<td>NFS Integration</td>
		<td>
<p>The code to seamlessly integrate with NFS servers has been commented out
with the HAVE_SHARE macro.  This code needs to be reviewed and updated
to integrate cleanly with the Linux NFS server implementation.  This
involves updating different config files than under OpenSolaris, and
putting a few fid hooks in place on the kernel side.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td>Samba Supported</td>
		<td>Beutner</td>
		<td>Medium</td>
		<td>SMB Integration</td>
		<td>
<p>The code to seamlessly integrate with SMB servers has been commented
out with the HAVE_SHARE macro.  This code needs to be reviewed and updated
to integrate cleanly with the Linux SMB server implementation.  I have
not yet scoped exactly what needs to be done to do this correctly.</p>
		</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>Hard</td>
		<td>Snapshot Directory (.zfs)</td>
		<td>
<p>While snapshots do work the .zfs snapshot directory has not yet been
implemented.  Snapshots can be manually mounted as needed with the
mount command, <i>mount -t zfs dataset@snap /mnt/snap</i>.  To implement
the .zfs snapshot directory a special .zfs inode must be created.
This inode will have custom hooks which allow it list available
snapshots as part of readdir(), and when a list is traversed the
dataset must be mounted on demand.  The mount itself will almost
certainly require an upcall to userspace to invoke the mount command
thus keeping all the config files properly updated.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Medium</td>
		<td>SELinux Integration</td>
		<td>
<p>Two things are still required for full SELinux integration.</p>

<p>First, we need convince the major distributions which enable
SELinux by default to add ZFS as a known filesystem type which
supports xattrs.  Without this all users will either need to rebuild
their SELinux policy, or use mountpoint labeling instead of the
preferred xattr labeling.</p>

<p>Second, the 'security.selinux' xattr should become a virtual xattr
(like system.richacl) which instead of storing the context in a
traditional xattr, stores the value in a System Attribute (SA)
attached to the dnode.  This will eliminate the extra xattr overhead
and will make it quick to access the selinux context.</p>
		</td>
	</tr>
	<tr>
		<td>
<a href="https://github.com/behlendorf/fstest">fstest</a> basic sanity tests
		</td>
		<td></td>
		<td>Medium</td>
		<td>ZFS Test Suite</td>
		<td>
<p>ZFS needs a test suite for the Posix layer (ZPL), block layer (ZVOL),
and tool chain (zfs, zpool, commands).  For the Posix layer many
indivual tests already exist (dbench, iozone, bonnie, etc)
but what we really want is a compliance test suite.  The
<a href="http://lwn.net/Articles/276617/">Linux POSIX file system
test suite</a> is one such framework which needs to be investigated.
Additionally, we need to validate the zfs/zpool commands to catch
regressions and the <a href="http://github.com/zfs-linux/test">
ZFS Test Suite</a> is probably a good fit for this.  Minimally,
we to document how any one can run these test suites on their
system.  It would be even better to automate it.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Easy</td>
		<td>Quotas</td>
		<td>
<p>Quota's have not been seriously investigated yet.  The code is all
in the ZFS port and compiling but it likely hasn't been wired in to
all the appropriate Linux infrastructure.  Exactly how it does or
doesn't work on Linux needs to be explored.</p>
		</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>Medium</td>
		<td>Mandatory Locking</td>
		<td>
<p>Support for mandatory locking has been disabled in the port.  The
Linux VFS already support most of this for us we just need to 
determine how to best leverage that support.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Hard</td>
		<td>Async IO</td>
		<td>
<p>The asynchronous IO handlers have not yet been implemented.  By default
ZFS handles everything asynchronously so this shouldn't be that hard but
it will take some care to implement the handlers.  Once they are in place
the synchronous handlers can removed and replaced with the generic
do_sync_read() and do_sync_write() handlers.  They simply call the
async hooks and then block resulting in a synchronous version.</p>
		</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>Hard</td>
		<td>Direct IO</td>
		<td>
<p>The direct IO handlers have not yet been implemented.  Supporting
direct IO would have been a problem a few years back because of how
ZFS copies everything in to the ARC cache.  However, recently ZFS
started supporting a zero-copy interface which we may be able to
leverage for direct IO support.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Hard</td>
		<td>mmap</td>
		<td>
<p>The current implementation fully supports mmap but the is some
additional cleanup in the implementation which is desirable.</p>

<p>First off it would be best to update the readpage and writepage
hooks to use zfs_fillpage and zfs_putapage respectively.  This was
not done initially because it was easier to use the existing
zpl_common_read/write() calls.  Updating zfs_fillpage and zfs_putapage
to be Linux VM friendly will take a little bit of work, but the end
result will be cleaner, faster code.  So it's worth doing.</p>

<p>Second, the Linux port still uses the same trick as Solaris for
mmap'ed files.  That is two copies of the mmap'ed regression are kept,
one in the page cache and one in the ARC.  The ZFS code is very careful
to keep the two synchronized but it is still a wart on the overall
design.  I believe I may be possible to unify the Linux page cache
with the ZFS ARC removing this design issue.  This would hopefully
cleanup the mmap() code considerably by removing the need to
synchronize the caches.</p>
		</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>Hard</td>
		<td>Fuse Implementation</td>
		<td>
<p>With all the ZPL code modified to be more Linux friendly it should
be fairly straight forward to enable building of zfs_vnops.c, zfs_vnops.c
zfs_dir.c, and zfs_znode in user space.  Then equivalents of the zpl_*
files called fuse_* can be implemented pretty easily.</p>

<p>There are two reasons I can think of offhand to support a fuse
implementation.  The first reason is portability.  While a fuse
implementation will probably never be as fast as a kernel implementation
it would simplify supporting older kernels, or perhaps other exotic
Linux environments.  Second, as a developer having a fulling working
fuse version could prove very handy for debugging and feature
development.</p>

<p>However, one of the major issues in supporting both implementations
will be deciding if the tool chain should be unified.  Does it make
sense to have a different set of zfs/zpool utils for the fuse version
vs the kernel version?  Should they be unified instead?  If so what
if your running the kernel and fuse version concurrently?</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Medium</td>
		<td>Remove Solaris VFS types</td>
		<td>
<p>One of the golden rules I maintained for the Linux port was to emulate
the Solaris types and interfaces as much as possible.  This is why there
is a SPL layer and why the bulk of the SPA/DMU code is the same under
Linux and Solaris.  That rule has served the ZFS on Linux port well but
there are times when it should be broken.</p>

<p> The most appropriate time is when the ZFS codes needs to directly
interface with a native Linux subsystem.  For example, the vdev_disk.c
source which allows ZFS to manipulate block devices was entirely rewritten
for Linux as was the ZVOL.  This same exception holds true for the Posix
layer (ZPL).</p>

<p>The Solaris VFS and the Linux VFS are so different it doesn't make any
sense to emulate the Solaris VFS under Linux.  Thus the ZPL implementation
in the ZFS on Linux port is a heavily reworked version of the Solaris ZPL.
The code remains largely the same but all the Solaris VFS specific types
and interfaces have been replaced with their Linux counterparts.  This is
best for the long term health and maintainability of the code.</p>

<p>While I've already eliminate many of the Solaris VFS'ish certain things
still remain.  Still on the chopping block is to remove the uio type
completely and replace it with the Linux equivalent.  Additionally, the
vattr_t type still needs to be phased out.</p>
		</td>
	</tr>
	<tr>
		<td>Done</td>
		<td>Behlendorf</td>
		<td>Easy</td>
		<td>zvol.c/vdev_disk.c compatibility code</td>
		<td>
<p>For legacy reasons the zvol.c and vdev_disk.c Linux compatibility
code ended up in the blkdev.h and vdev_disk.h headers respectively.
While there are worse places for this code to live there is going
to be additional compatibility code added for the ZPL.  All of the
Linux compatibility code should be moved to <i>include/linux/*_compat.h</i>
header files for consistency.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Medium</td>
		<td>Inode/Znode Refactoring</td>
		<td>
<p>The current implementation preserves the full Solaris znode.  This znode
however has several fields which are redundant with a Linux inode (atime,
size, uid, gid, mode, gen).  This means that the code first updates the
znode with off disk values, and then the inode is updated from the znode.
There's really no reason to keep this for Linux.  The redundant znode
value should be dropped and the inode updated directly.</p>
		</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>Easy</td>
		<td>FWRITE/FREAD</td>
		<td>
<p>Related to removing the Solaris VFS'isms we should also replace
certain Solaris types in the ZPL.  For example the FWRITE, FREAD, and
friends can be directly replaced with their Linux equivalent.  These
are the constants which will be passed through the system calls so
it make the most sense to use their Linux names.</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td>
<a href="https://launchpad.net/~dajhorn/+archive/zfs">Packages available</a>
		</td>
		<td>Darik Horn</td>
		<td>Easy</td>
		<td>Debian Packaging</td>
		<td>
<p>Clean native Debian style packaging should be supported.  This should
be relatively straight forward for someone familiar with the Debian
packaging guidelines, and the existing rpm spec files to use as a
reference.</p>
		</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>Hard</td>
		<td>Security Policy</td>
		<td>
<p>Currently the Solaris secpolicy_* hooks are all disabled.  This isn't
as bad as it sounds through because almost all of the standard enforcement
is handled for us automatically by the Linux VFS.  However, there are still
a couple places, such as file attributes, where we need this code to be in
place and working.  We should be able to pull over the policy source from
OpenSolaris and use that with some modification.  However, there are bound
to be some got'chas which will take time to resolve..</p>
		</td>
	</tr>
	<tr bgcolor="#dddddd">
		<td></td>
		<td></td>
		<td>Medium</td>
		<td>File attributes</td>
		<td>
<p>Internally ZFS already has all the code in place to correctly handle the
file attributes (immutable/read-only/etc).  They should even be enforced if
you import a pool from a different platform with these attributes set.
What's missing are the needed interfaces to userspace to manipulate them.
Under Linux this is typically done with the <i>lsattr/chattr</i> utilities.
However, the file attributes supported by these tools only partly overlap
with the set of Solaris file attributes.  That means we either need to
write our own tool to manipulate them on Linux, or we need to port the
Solaris utility.  A prerequisite for this work will be to get the security
policy handlers working to ensure only authorized users can manipulate
the file attributes.</p>
		</td>
	</tr>
</table>

</center>
</body>
</html>
